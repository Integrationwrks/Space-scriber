{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Integrationwrks/Space-scriber/blob/main/Twitter_Spaces_Transcription_App_with_onscreen_text_display.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, here are the steps on how to transcribe audio from Twitter Spaces and allow an option for on screen text display word by word transcription with descriptions of who is speaking to allow a person with a hearing impairment follow along with a conversation in Twitter Spaces. Make it 508 compliant, make it user friendly, and also create a plugin for Chrome:\n",
        "\n",
        "1. Import the necessary libraries."
      ],
      "metadata": {
        "id": "Ag1A7rSP9Skk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from google.assistant.library import Assistant\n",
        "from google.assistant.library.event import EventType, SessionEndedReason\n",
        "\n",
        "import pyautogui\n",
        "import time\n",
        "\n",
        "from gtts import gTTS\n",
        "\n",
        "import speech_recognition as sr\n",
        "\n",
        "import webbrowser\n",
        "\n",
        "from bs4 import BeautifulSoup"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "g9u_kyBz9Skp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create a new `GoogleAssistant` object and pass in your credentials."
      ],
      "metadata": {
        "id": "4jlM8UE09Skq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = Assistant()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "fdUVdorB9Skr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a new `Session` object and pass in the `GoogleAssistant` object."
      ],
      "metadata": {
        "id": "1sKMxrgn9Sks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session = assistant.create_session()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "hu5_TUVG9Skt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Start the session and listen for events."
      ],
      "metadata": {
        "id": "GQoy2Y-p9Skt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session.start()\n",
        "\n",
        "while True:\n",
        "  # Wait for an event.\n",
        "  event = session.wait_for_event()\n",
        "\n",
        "  # Handle the event.\n",
        "  if event.type == EventType.SESSION_STARTED:\n",
        "    print(\"Session started\")\n",
        "  elif event.type == EventType.INTENT_RECEIVED:\n",
        "    print(\"Intent received:\", event.intent.name)\n",
        "\n",
        "    if event.intent.name == \"actions.intent.TEXT\":\n",
        "      response = Bard.query(event.intent.data)\n",
        "      session_started_event.session.send_text(response)\n",
        "\n",
        "  elif event.type == EventType.SESSION_ENDED:\n",
        "    print(\"Session ended with reason:\", event.reason)\n",
        "\n",
        "    # If the event is a `SessionEnded` event with a reason of `END_OF_FLOW`,\n",
        "    # the session is finished and you can exit the loop.\n",
        "    if event.type == EventType.SESSION_ENDED and event.reason == SessionEndedReason.END_OF_FLOW:\n",
        "      break\n",
        "\n",
        "  # If the event is a `TRANSCRIPT_STARTED` event, start transcribing the audio.\n",
        "  if event.type == EventType.TRANSCRIPT_STARTED:\n",
        "    pyautogui.click(100, 100)\n",
        "\n",
        "    # Create a new `Text` object to store the transcript.\n",
        "    text = \"\"\n",
        "\n",
        "    # Start a timer to keep track of the time it takes to transcribe the audio.\n",
        "    start_time = time.time()\n",
        "\n",
        "  # If the event is a `TRANSCRIPT_UPDATE` event, update the transcript.\n",
        "  elif event.type == EventType.TRANSCRIPT_UPDATE:\n",
        "    text += event.transcript\n",
        "\n",
        "  # If the event is a `TRANSCRIPT_END` event, stop transcribing the audio and print the transcript.\n",
        "  elif event.type == EventType.TRANSCRIPT_END:\n",
        "    pyautogui.click(100, 100)\n",
        "\n",
        "    # Get the time it took to transcribe the audio.\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Print the transcript.\n",
        "    print(text)\n",
        "\n",
        "    # Print the time it took to transcribe the audio.\n",
        "    print(\"Transcription time:\", end_time - start_time)\n",
        "\n",
        "    # If the user wants to output the transcript to a pdf or google doc, do so.\n",
        "    if event.intent.data == \"pdf\":\n",
        "      # Write the transcript to a pdf file.\n",
        "      with open(\"transcript.pdf\", \"w\") as f:\n",
        "        f.write(text)\n",
        "    elif event.intent.data == \"google doc\":\n",
        "      # Write the transcript to a google doc.\n",
        "      with open(\"transcript.docx\", \"w\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "    # Create a new `SpeechRecognition` object.\n",
        "    rec = sr.Recognizer()\n",
        "\n",
        "    # Create a new `Microphone` object.\n",
        "    microphone = sr.Microphone()\n",
        "\n",
        "    # Start listening for audio.\n",
        "    with microphone as source:\n",
        "      audio = rec.listen(source)\n",
        "\n",
        "    # Transcribe the audio.\n",
        "    try:\n",
        "      text = rec.recognize_google(audio)\n",
        "    except sr"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "2dLQxM0Q9Sku"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}